# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12GdttDby3roPiFobewKTgJk88hJJMnTk
"""

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Record your voice directly in Colab
from google.colab import output
output.eval_js('new AudioContext()')

# Record for 5 seconds
print("Recording... Speak now!")
audio_data = output.eval_js('new Promise(async resolve => { const stream = await navigator.mediaDevices.getUserMedia({ audio: true }); const recorder = new MediaRecorder(stream); const chunks = []; recorder.ondataavailable = e => chunks.push(e.data); recorder.onstop = () => resolve(new Blob(chunks)); recorder.start(); await new Promise(resolve => setTimeout(resolve, 5000)); recorder.stop(); })')

# Save the recording as an MP3 file
with open("input.mp3", "wb") as f:
    f.write(audio_data.to_bytes())

# Play the recording
print("Playing your recording...")
display(Audio("input.mp3", autoplay=True))

# Transcribe your voice
audio_file = open("input.mp3", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.mp3'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Play the recording
print("Playing your recording...")
display(Audio("input.mp3", autoplay=True))

# Transcribe your voice
audio_file = open("input.mp3", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.mp3'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Play the recording
print("Playing your recording...")
display(Audio("input.mp3", autoplay=True))

# Transcribe your voice
audio_file = open("input.mp3", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Convert M4A to WAV
audio = AudioSegment.from_file("input.m4a", format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Transcribe your voice
audio_file = open("input.wav", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Convert M4A to WAV
audio = AudioSegment.from_file("input.m4a", format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Transcribe your voice
audio_file = open("input.wav", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Transcribe your voice
audio_file = open("input.wav", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Transcribe your voice
audio_file = open("input.wav", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Transcribe your voice
audio_file = open("input.wav", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install openai pydub
from openai import OpenAI
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Set up OpenAI (use your new key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Transcribe your voice
audio_file = open("input.wav", "rb")
transcript = client.audio.transcriptions.create(
  model="whisper-1",
  file=audio_file
)
print("You said:", transcript.text)

!pip install whisper

import whisper

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
print("You said:", result["text"])

import whisper

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
print("You said:", result["text"])

!pip install -U openai-whisper

!pip install -U openai-whisper
!pip install pydub
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
print("You said:", result["text"])

!pip install -U openai-whisper
!pip install pydub
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response
import openai
openai.api_key = "YOUR_OPENAI_KEY"  # Replace with your OpenAI key

response = openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a supportive and empathetic friend."},
    {"role": "user", "content": user_message}
  ]
)

ai_response = response.choices[0].message.content
print("AI says:", ai_response)

!pip install -U openai-whisper
!pip install pydub
!pip install openai==1.0.0  # Ensure the latest OpenAI library is installed
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
from openai import OpenAI  # Import the new OpenAI client

# Set up OpenAI (use your key)
client = OpenAI(api_key="YOUR_OPENAI_KEY")  # Replace with your OpenAI key

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a supportive and empathetic friend."},
    {"role": "user", "content": user_message}
  ]
)

ai_response = response.choices[0].message.content
print("AI says:", ai_response)

!pip install -U openai-whisper
!pip install pydub
!pip install openai==1.0.0  # Ensure the latest OpenAI library is installed
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
from openai import OpenAI  # Import the new OpenAI client

# Set up OpenAI (use your new key)
client = OpenAI(api_key="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")

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response
response = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a supportive and empathetic friend."},
    {"role": "user", "content": user_message}
  ]
)

ai_response = response.choices[0].message.content
print("AI says:", ai_response)

!pip install -U openai-whisper
!pip install pydub
!pip install openai==0.28  # Use the older OpenAI library to avoid quota issues
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
import openai  # Use the older OpenAI library

# Set up OpenAI (use your key)
openai.api_key = "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"

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment

I ran the updated code!

!pip install -U openai-whisper
!pip install pydub
!pip install openai==0.28  # Use the older OpenAI library to avoid quota issues
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
import openai  # Use the older OpenAI library

# Set up OpenAI (use your key)
openai.api_key = "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"

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response
response = openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": "You are a supportive and empathetic friend."},
    {"role": "user", "content": user_message}
  ]
)

ai_response = response.choices[0].message.content
print("AI says:", ai_response)

!pip install whisper

import whisper

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
print("You said:", result["text"])

!pip install transformers

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
from transformers import pipeline  # For AI responses

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response using Hugging Face
chatbot = pipeline("conversational", model="microsoft/DialoGPT-medium")
response = chatbot(user_message)[0]["generated_text"]
print("AI says:", response)

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
from transformers import pipeline  # For AI responses

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response using Hugging Face
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")
response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
print("AI says:", response)

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
!pip install gtts  # For text-to-speech
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
from transformers import pipeline  # For AI responses
from gtts import gTTS  # For text-to-speech

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response using Hugging Face
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")
response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
print("AI says:", response)

# Convert AI response to speech
tts = gTTS(response, lang="en")
tts.save("ai_response.mp3")

# Play the AI's response
print("Playing AI's response...")
display(Audio("ai_response.mp3", autoplay=True))

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
!pip install gtts  # For text-to-speech
!pip install requests  # For D-ID API
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display
import os
from transformers import pipeline  # For AI responses
from gtts import gTTS  # For text-to-speech
import requests  # For D-ID API

# Upload your voice recording
print("Please record your voice using any app (e.g., Voice Memos on iPhone) and save it as 'input.m4a'.")
print("Upload the file to Colab:")

# Upload the file
from google.colab import files
uploaded = files.upload()

# Get the uploaded filename
uploaded_filename = list(uploaded.keys())[0]

# Convert M4A to WAV
audio = AudioSegment.from_file(uploaded_filename, format="m4a")
audio.export("input.wav", format="wav")

# Play the recording
print("Playing your recording...")
display(Audio("input.wav", autoplay=True))

# Load Whisper model
model = whisper.load_model("base")

# Transcribe your voice
result = model.transcribe("input.wav")
user_message = result["text"]
print("You said:", user_message)

# Generate AI response using Hugging Face
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")
response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
print("AI says:", response)

# Convert AI response to speech
tts = gTTS(response, lang="en")
tts.save("ai_response.mp3")

# Play the AI's response
print("Playing AI's response...")
display(Audio("ai_response.mp3", autoplay=True))

# Create an animated avatar with D-ID
import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key

# Upload an avatar image (e.g., a PNG file)
print("Please upload an avatar image (e.g., a PNG file):")
avatar_image = files.upload()
avatar_filename = list(avatar_image.keys())[0]

# Send request to D-ID API
url = "https://api.d-id.com/talks"
headers = {
    "Authorization": f"Basic {DID_API_KEY}",  # Use Basic Auth for D-ID
    "Content-Type": "application/json"
}
data = {
    "script": {
        "type": "audio",
        "audio_url": "ai_response.mp3"
    },
    "source_url": avatar_filename
}

response = requests.post(url, headers=headers, json=data)
if response.status_code == 200:
    video_url = response.json()["url"]
    print("Avatar video created! Download it here:", video_url)
else:
    print("Error creating avatar video:", response.text)

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
!pip install gtts  # For text-to-speech
!pip install requests  # For D-ID API
!pip install sounddevice  # For real-time voice detection
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display, clear_output
import os
from transformers import pipeline  # For AI responses
from gtts import gTTS  # For text-to-speech
import requests  # For D-ID API
import sounddevice as sd  # For real-time voice detection
import numpy as np
import wave

# Set up Whisper model
model = whisper.load_model("base")

# Set up Hugging Face pipeline for AI responses
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")

# Set up D-ID API key
import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key

# Function to record audio in real-time
def record_audio(duration=5, sample_rate=16000):
    print("Listening... Speak now!")
    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype="int16")
    sd.wait()  # Wait until recording is finished
    return audio.flatten()

# Function to save audio to a file
def save_audio(audio, filename, sample_rate=16000):
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio.tobytes())

# Function to transcribe audio
def transcribe_audio(filename):
    result = model.transcribe(filename)
    return result["text"]

# Function to generate AI response
def generate_response(user_message):
    response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
    return response

# Function to convert text to speech
def text_to_speech(text, filename):
    tts = gTTS(text, lang="en")
    tts.save(filename)

# Function to create animated avatar
def create_avatar(audio_file, avatar_image):
    url = "https://api.d-id.com/talks"
    headers = {
        "Authorization": f"Basic {DID_API_KEY}",  # Use Basic Auth for D-ID
        "Content-Type": "application/json"
    }
    data = {
        "script": {
            "type": "audio",
            "audio_url": audio_file
        },
        "source_url": avatar_image
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()["url"]
    else:
        raise Exception("Error creating avatar video:", response.text)

# Main loop for continuous conversation
print("Starting the app... Speak to your AI friend!")
while True:
    try:
        # Step 1: Record user's voice
        audio_data = record_audio()
        save_audio(audio_data, "input.wav")

        # Step 2: Transcribe the audio
        user_message = transcribe_audio("input.wav")
        print("You said:", user_message)

        # Step 3: Generate AI response
        ai_response = generate_response(user_message)
        print("AI says:", ai_response)

        # Step 4: Convert AI response to speech
        text_to_speech(ai_response, "ai_response.mp3")

        # Step 5: Play the AI's response
        display(Audio("ai_response.mp3", autoplay=True))

        # Step 6: Create and display animated avatar
        avatar_url = create_avatar("ai_response.mp3", "avatar.png")  # Replace with your avatar image
        print("Avatar video created! Download it here:", avatar_url)

    except KeyboardInterrupt:
        print("Exiting the app...")
        break
    except Exception as e:
        print("An error occurred:", e)

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
!pip install gtts  # For text-to-speech
!pip install requests  # For D-ID API
!pip install sounddevice  # For real-time voice detection
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display, clear_output
import os
from transformers import pipeline  # For AI responses
from gtts import gTTS  # For text-to-speech
import requests  # For D-ID API
import sounddevice as sd  # For real-time voice detection
import numpy as np
import wave

# Set up Whisper model
model = whisper.load_model("base")

# Set up Hugging Face pipeline for AI responses
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")

# Set up D-ID API key
import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key

# Function to record audio in real-time
def record_audio(duration=5, sample_rate=16000):
    print("Listening... Speak now!")
    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype="int16")
    sd.wait()  # Wait until recording is finished
    return audio.flatten()

# Function to save audio to a file
def save_audio(audio, filename, sample_rate=16000):
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio.tobytes())

# Function to transcribe audio
def transcribe_audio(filename):
    result = model.transcribe(filename)
    return result["text"]

# Function to generate AI response
def generate_response(user_message):
    response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
    return response

# Function to convert text to speech
def text_to_speech(text, filename):
    tts = gTTS(text, lang="en")
    tts.save(filename)

# Function to create animated avatar
def create_avatar(audio_file, avatar_image):
    url = "https://api.d-id.com/talks"
    headers = {
        "Authorization": f"Basic {DID_API_KEY}",  # Use Basic Auth for D-ID
        "Content-Type": "application/json"
    }
    data = {
        "script": {
            "type": "audio",
            "audio_url": audio_file
        },
        "source_url": avatar_image
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()["url"]
    else:
        raise Exception("Error creating avatar video:", response.text)

# Main loop for continuous conversation
print("Starting the app... Speak to your AI friend!")
while True:
    try:
        # Step 1: Record user's voice
        audio_data = record_audio()
        save_audio(audio_data, "input.wav")

        # Step 2: Transcribe the audio
        user_message = transcribe_audio("input.wav")
        print("You said:", user_message)

        # Step 3: Generate AI response
        ai_response = generate_response(user_message)
        print("AI says:", ai_response)

        # Step 4: Convert AI response to speech
        text_to_speech(ai_response, "ai_response.mp3")

        # Step 5: Play the AI's response
        display(Audio("ai_response.mp3", autoplay=True))

        # Step 6: Create and display animated avatar
        avatar_url = create_avatar("ai_response.mp3", "avatar.png")  # Replace with your avatar image
        print("Avatar video created! Download it here:", avatar_url)

    except KeyboardInterrupt:
        print("Exiting the app...")
        break
    except Exception as e:
        print("An error occurred:", e)

!apt-get install -y libportaudio2

# Step 1: Install PortAudio
!apt-get install -y libportaudio2

# Step 2: Install required Python packages
!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
!pip install gtts  # For text-to-speech
!pip install requests  # For D-ID API
!pip install sounddevice  # For real-time voice detection

# Step 3: Import libraries
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display, clear_output
import os
from transformers import pipeline  # For AI responses
from gtts import gTTS  # For text-to-speech
import requests  # For D-ID API
import sounddevice as sd  # For real-time voice detection
import numpy as np
import wave

# Set up Whisper model
model = whisper.load_model("base")

# Set up Hugging Face pipeline for AI responses
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")

# Set up D-ID API key
import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key

# Function to record audio in real-time
def record_audio(duration=5, sample_rate=16000):
    print("Listening... Speak now!")
    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype="int16")
    sd.wait()  # Wait until recording is finished
    return audio.flatten()

# Function to save audio to a file
def save_audio(audio, filename, sample_rate=16000):
    with wave.open(filename, "wb") as wf:
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(sample_rate)
        wf.writeframes(audio.tobytes())

# Function to transcribe audio
def transcribe_audio(filename):
    result = model.transcribe(filename)
    return result["text"]

# Function to generate AI response
def generate_response(user_message):
    response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
    return response

# Function to convert text to speech
def text_to_speech(text, filename):
    tts = gTTS(text, lang="en")
    tts.save(filename)

# Function to create animated avatar
def create_avatar(audio_file, avatar_image):
    url = "https://api.d-id.com/talks"
    headers = {
        "Authorization": f"Basic {DID_API_KEY}",  # Use Basic Auth for D-ID
        "Content-Type": "application/json"
    }
    data = {
        "script": {
            "type": "audio",
            "audio_url": audio_file
        },
        "source_url": avatar_image
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()["url"]
    else:
        raise Exception("Error creating avatar video:", response.text)

# Main loop for continuous conversation
print("Starting the app... Speak to your AI friend!")
while True:
    try:
        # Step 1: Record user's voice
        audio_data = record_audio()
        save_audio(audio_data, "input.wav")

        # Step 2: Transcribe the audio
        user_message = transcribe_audio("input.wav")
        print("You said:", user_message)

        # Step 3: Generate AI response
        ai_response = generate_response(user_message)
        print("AI says:", ai_response)

        # Step 4: Convert AI response to speech
        text_to_speech(ai_response, "ai_response.mp3")

        # Step 5: Play the AI's response
        display(Audio("ai_response.mp3", autoplay=True))

        # Step 6: Create and display animated avatar
        avatar_url = create_avatar("ai_response.mp3", "avatar.png")  # Replace with your avatar image
        print("Avatar video created! Download it here:", avatar_url)

    except KeyboardInterrupt:
        print("Exiting the app...")
        break
    except Exception as e:
        print("An error occurred:", e)

!pip install -U openai-whisper
!pip install pydub
!pip install transformers  # For Hugging Face API
!pip install gtts  # For text-to-speech
!pip install requests  # For D-ID API
import whisper
from pydub import AudioSegment
from pydub.playback import play
from IPython.display import Audio, display, clear_output
import os
from transformers import pipeline  # For AI responses
from gtts import gTTS  # For text-to-speech
import requests  # For D-ID API
import numpy as np
import wave

# Set up Whisper model
model = whisper.load_model("base")

# Set up Hugging Face pipeline for AI responses
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")

# Set up D-ID API key
import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key

# Function to record audio using Colab's built-in recorder
def record_audio(duration=5):
    from google.colab import output
    print("Listening... Speak now!")
    audio_data = output.eval_js('new Promise(async resolve => { const stream = await navigator.mediaDevices.getUserMedia({ audio: true }); const recorder = new MediaRecorder(stream); const chunks = []; recorder.ondataavailable = e => chunks.push(e.data); recorder.onstop = () => resolve(new Blob(chunks)); recorder.start(); await new Promise(resolve => setTimeout(resolve, duration * 1000)); recorder.stop(); })')
    return audio_data

# Function to save audio to a file
def save_audio(audio_data, filename):
    with open(filename, "wb") as f:
        f.write(audio_data.to_bytes())

# Function to transcribe audio
def transcribe_audio(filename):
    result = model.transcribe(filename)
    return result["text"]

# Function to generate AI response
def generate_response(user_message):
    response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
    return response

# Function to convert text to speech
def text_to_speech(text, filename):
    tts = gTTS(text, lang="en")
    tts.save(filename)

# Function to create animated avatar
def create_avatar(audio_file, avatar_image):
    url = "https://api.d-id.com/talks"
    headers = {
        "Authorization": f"Basic {DID_API_KEY}",  # Use Basic Auth for D-ID
        "Content-Type": "application/json"
    }
    data = {
        "script": {
            "type": "audio",
            "audio_url": audio_file
        },
        "source_url": avatar_image
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        return response.json()["url"]
    else:
        raise Exception("Error creating avatar video:", response.text)

# Main loop for continuous conversation
print("Starting the app... Speak to your AI friend!")
while True:
    try:
        # Step 1: Record user's voice
        audio_data = record_audio()
        save_audio(audio_data, "input.wav")

        # Step 2: Transcribe the audio
        user_message = transcribe_audio("input.wav")
        print("You said:", user_message)

        # Step 3: Generate AI response
        ai_response = generate_response(user_message)
        print("AI says:", ai_response)

        # Step 4: Convert AI response to speech
        text_to_speech(ai_response, "ai_response.mp3")

        # Step 5: Play the AI's response
        display(Audio("ai_response.mp3", autoplay=True))

        # Step 6: Create and display animated avatar
        avatar_url = create_avatar("ai_response.mp3", "avatar.png")  # Replace with your avatar image
        print("Avatar video created! Download it here:", avatar_url)

    except KeyboardInterrupt:
        print("Exiting the app...")
        break
    except Exception as e:
        print("An error occurred:", e)

!pip install streamlit

import streamlit as st
import whisper
from transformers import pipeline
from gtts import gTTS
import requests
import os
from io import BytesIO

# Set up Whisper model
model = whisper.load_model("base")

# Set up Hugging Face pipeline for AI responses
generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")

# Set up D-ID API key
import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key

# Streamlit app
st.title("AI Friend App 🎙️🤖")
st.write("Speak to your AI friend and see it respond with an animated avatar!")

# Record audio
audio_file = st.file_uploader("Upload an audio file (e.g., .wav, .mp3)", type=["wav", "mp3"])

if audio_file:
    # Save the uploaded file
    with open("input.wav", "wb") as f:
        f.write(audio_file.read())

    # Transcribe the audio
    result = model.transcribe("input.wav")
    user_message = result["text"]
    st.write("You said:", user_message)

    # Generate AI response
    ai_response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
    st.write("AI says:", ai_response)

    # Convert AI response to speech
    tts = gTTS(ai_response, lang="en")
    tts.save("ai_response.mp3")

    # Play the AI's response
    st.audio("ai_response.mp3", format="audio/mp3")

    # Create animated avatar
    avatar_image = "avatar.png"  # Replace with your avatar image
    url = "https://api.d-id.com/talks"
    headers = {
        "Authorization": f"Basic {DID_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "script": {
            "type": "audio",
            "audio_url": "ai_response.mp3"
        },
        "source_url": avatar_image
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        video_url = response.json()["url"]
        st.write("Avatar video created! Download it here:", video_url)
    else:
        st.error("Error creating avatar video:", response.text)

!streamlit run app.py

!streamlit run app.py

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import whisper
# from transformers import pipeline
# from gtts import gTTS
# import requests
# import os
# from io import BytesIO
# 
# # Set up Whisper model
# model = whisper.load_model("base")
# 
# # Set up Hugging Face pipeline for AI responses
# generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")
# 
# # Set up D-ID API key
# import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key
# 
# # Streamlit app
# st.title("AI Friend App 🎙️🤖")
# st.write("Speak to your AI friend and see it respond with an animated avatar!")
# 
# # Record audio
# audio_file = st.file_uploader("Upload an audio file (e.g., .wav, .mp3)", type=["wav", "mp3"])
# 
# if audio_file:
#     # Save the uploaded file
#     with open("input.wav", "wb") as f:
#         f.write(audio_file.read())
# 
#     # Transcribe the audio
#     result = model.transcribe("input.wav")
#     user_message = result["text"]
#     st.write("You said:", user_message)
# 
#     # Generate AI response
#     ai_response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
#     st.write("AI says:", ai_response)
# 
#     # Convert AI response to speech
#     tts = gTTS(ai_response, lang="en")
#     tts.save("ai_response.mp3")
# 
#     # Play the AI's response
#     st.audio("ai_response.mp3", format="audio/mp3")
# 
#     # Create animated avatar
#     avatar_image = "avatar.png"  # Replace with your avatar image
#     url = "https://api.d-id.com/talks"
#     headers = {
#         "Authorization": f"Basic {DID_API_KEY}",
#         "Content-Type": "application/json"
#     }
#     data = {
#         "script": {
#             "type": "audio",
#             "audio_url": "ai_response.mp3"
#         },
#         "source_url": avatar_image
#     }
#     response = requests.post(url, headers=headers, json=data)
#     if response.status_code == 200:
#         video_url = response.json()["url"]
#         st.write("Avatar video created! Download it here:", video_url)
#     else:
#         st.error("Error creating avatar video:", response.text)

!streamlit run app.py

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import whisper
# from transformers import pipeline
# from gtts import gTTS
# import requests
# import os
# from io import BytesIO
# 
# # Set up Whisper model
# model = whisper.load_model("base")
# 
# # Set up Hugging Face pipeline for AI responses
# generator = pipeline("text-generation", model="microsoft/DialoGPT-medium")
# 
# # Set up D-ID API key
# import os   DID_API_KEY = os.getenv("DID_API_KEY")  # Read from environment variable  # Your D-ID API key
# 
# # Streamlit app
# st.title("AI Friend App 🎙️🤖")
# st.write("Speak to your AI friend and see it respond with an animated avatar!")
# 
# # Record audio
# audio_file = st.file_uploader("Upload an audio file (e.g., .wav, .mp3)", type=["wav", "mp3"])
# 
# if audio_file:
#     # Save the uploaded file
#     with open("input.wav", "wb") as f:
#         f.write(audio_file.read())
# 
#     # Transcribe the audio
#     result = model.transcribe("input.wav")
#     user_message = result["text"]
#     st.write("You said:", user_message)
# 
#     # Generate AI response
#     ai_response = generator(user_message, max_length=50, num_return_sequences=1)[0]["generated_text"]
#     st.write("AI says:", ai_response)
# 
#     # Convert AI response to speech
#     tts = gTTS(ai_response, lang="en")
#     tts.save("ai_response.mp3")
# 
#     # Play the AI's response
#     st.audio("ai_response.mp3", format="audio/mp3")
# 
#     # Create animated avatar
#     avatar_image = "avatar.png"  # Replace with your avatar image
#     url = "https://api.d-id.com/talks"
#     headers = {
#         "Authorization": f"Basic {DID_API_KEY}",
#         "Content-Type": "application/json"
#     }
#     data = {
#         "script": {
#             "type": "audio",
#             "audio_url": "ai_response.mp3"
#         },
#         "source_url": avatar_image
#     }
#     response = requests.post(url, headers=headers, json=data)
#     if response.status_code == 200:
#         video_url = response.json()["url"]
#         st.write("Avatar video created! Download it here:", video_url)
#     else:
#         st.error("Error creating avatar video:", response.text)

